{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0euZQ65rMPMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b1f7bd-dab6-41f2-8003-d77033f24846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install -U scikit-learn pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# --------------------- Helpers ---------------------\n",
        "\n",
        "POSSIBLE_TARGET_NAMES = [\n",
        "    \"target\",\"label\",\"class\",\"diagnosis\",\"lung_cancer\",\"lung-cancer\",\"LUNG_CANCER\",\n",
        "    \"Outcome\",\"outcome\",\"Result\",\"result\"\n",
        "]\n",
        "\n",
        "def guess_target_column(df: pd.DataFrame) -> str:\n",
        "    # 1) by known names (case-insensitive)\n",
        "    lower = {c.lower(): c for c in df.columns}\n",
        "    for name in POSSIBLE_TARGET_NAMES:\n",
        "        if name.lower() in lower:\n",
        "            return lower[name.lower()]\n",
        "    # 2) choose a column with few unique values (2-10) as a likely label\n",
        "    candidates = [c for c in df.columns if df[c].nunique()<=10 and df[c].nunique()>=2]\n",
        "    if candidates:\n",
        "        return candidates[-1]\n",
        "    # 3) fallback: last column\n",
        "    return df.columns[-1]\n",
        "\n",
        "def build_preprocessor(num_cols, cat_cols, for_pca=False):\n",
        "    \"\"\"Create a ColumnTransformer.\n",
        "    If for_pca=True, we'll output dense to feed PCA.\n",
        "    \"\"\"\n",
        "    num_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "    pre = ColumnTransformer([\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "        (\"cat\", cat_pipe, cat_cols)\n",
        "    ])\n",
        "    if for_pca:\n",
        "        # Convert sparse to dense so PCA can work\n",
        "        pre = Pipeline([\n",
        "            (\"ct\", pre),\n",
        "            (\"to_dense\", FunctionTransformer(lambda x: x.toarray(), accept_sparse=True))\n",
        "        ])\n",
        "    return pre\n",
        "\n",
        "def evaluate(model, X_test, y_test, prefix=\"\"):\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec_macro = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    rec_macro  = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1_macro   = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": prec_macro,\n",
        "        \"recall_macro\": rec_macro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"report_text\": report\n",
        "    }\n",
        "\n",
        "def get_feature_names_after_preprocess(pipeline: Pipeline, num_cols, cat_cols):\n",
        "    \"\"\"\n",
        "    Returns list of feature names after the ColumnTransformer in the baseline model.\n",
        "    \"\"\"\n",
        "    pre = pipeline.named_steps[\"preprocess\"]\n",
        "    # If wrapped into Pipeline, unwrap\n",
        "    if isinstance(pre, Pipeline):\n",
        "        pre = pre.named_steps[\"ct\"] if \"ct\" in pre.named_steps else pre\n",
        "\n",
        "    names = []\n",
        "    # numeric names\n",
        "    names.extend(list(num_cols))\n",
        "    # categorical one-hot names\n",
        "    ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
        "    ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
        "    names.extend(ohe_names)\n",
        "    return names\n",
        "\n",
        "def save_confusion_matrix(cm: np.ndarray, path: Path):\n",
        "    df_cm = pd.DataFrame(cm)\n",
        "    df_cm.to_csv(path, index=False)\n",
        "\n",
        "# --------------------- Main ---------------------\n",
        "\n",
        "def main():\n",
        "    # Define arguments directly\n",
        "    data_file = \"/content/Lung_Cancer_dataset.csv\"\n",
        "    target_column = None # or specify a column name like \"lung_cancer\"\n",
        "    test_size_ratio = 0.2\n",
        "    random_state_value = 42\n",
        "\n",
        "    data_path = Path(data_file)\n",
        "    assert data_path.exists(), f\"Data file not found: {data_path.resolve()}\"\n",
        "\n",
        "    out_dir = Path(\"outputs\")\n",
        "    out_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Load\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"Loaded: {data_path} -> shape {df.shape}\")\n",
        "    # Pick target\n",
        "    target = target_column or guess_target_column(df)\n",
        "    assert target in df.columns, f\"Target column '{target}' not found in CSV. Use --target to set it.\"\n",
        "    print(f\"Using target column: {target}\")\n",
        "    print(\"Class distribution:\")\n",
        "    print(df[target].value_counts(dropna=False))\n",
        "\n",
        "    # Features/label split\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "\n",
        "    # Identify column types\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    print(f\"Numeric cols: {len(num_cols)} | Categorical cols: {len(cat_cols)}\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size_ratio, random_state=random_state_value, stratify=y if y.nunique()<=20 else None\n",
        "    )\n",
        "\n",
        "    # ---------- Baseline Decision Tree ----------\n",
        "    baseline_pre = build_preprocessor(num_cols, cat_cols, for_pca=False)\n",
        "    baseline_clf = Pipeline([\n",
        "        (\"preprocess\", baseline_pre),\n",
        "        (\"dt\", DecisionTreeClassifier(random_state=random_state_value))\n",
        "    ])\n",
        "    baseline_clf.fit(X_train, y_train)\n",
        "    base_eval = evaluate(baseline_clf, X_test, y_test, prefix=\"baseline\")\n",
        "\n",
        "    # Save baseline artifacts\n",
        "    (out_dir/\"baseline_report.txt\").write_text(base_eval[\"report_text\"])\n",
        "    save_confusion_matrix(base_eval[\"confusion_matrix\"], out_dir/\"confusion_matrix_baseline.csv\")\n",
        "\n",
        "    # Feature importance (after OHE)\n",
        "    try:\n",
        "        feat_names = get_feature_names_after_preprocess(baseline_clf, num_cols, cat_cols)\n",
        "        importances = baseline_clf.named_steps[\"dt\"].feature_importances_\n",
        "        feat_imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
        "        feat_imp_df = feat_imp_df.sort_values(\"importance\", ascending=False)\n",
        "        feat_imp_df.head(50).to_csv(out_dir/\"feature_importance_baseline.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(\"Could not extract feature importances (skipping):\", e)\n",
        "\n",
        "    # ---------- PCA + Decision Tree ----------\n",
        "    pca_pre = build_preprocessor(num_cols, cat_cols, for_pca=True)\n",
        "    pca_clf = Pipeline([\n",
        "        (\"preprocess\", pca_pre),\n",
        "        (\"pca\", PCA(n_components=0.95, random_state=random_state_value)),\n",
        "        (\"dt\", DecisionTreeClassifier(random_state=random_state_value))\n",
        "    ])\n",
        "    pca_clf.fit(X_train, y_train)\n",
        "    pca_eval = evaluate(pca_clf, X_test, y_test, prefix=\"pca\")\n",
        "\n",
        "    # Save PCA artifacts\n",
        "    (out_dir/\"pca_report.txt\").write_text(pca_eval[\"report_text\"])\n",
        "    save_confusion_matrix(pca_eval[\"confusion_matrix\"], out_dir/\"confusion_matrix_pca.csv\")\n",
        "\n",
        "    # PCA details\n",
        "    n_components = pca_clf.named_steps[\"pca\"].n_components_\n",
        "    explained = pca_clf.named_steps[\"pca\"].explained_variance_ratio_.sum()\n",
        "    print(f\"PCA kept components: {n_components} | Total variance explained: {explained:.3f}\")\n",
        "\n",
        "    # ---------- Comparison table ----------\n",
        "    compare = pd.DataFrame([\n",
        "        {\"model\":\"Baseline DT\",\n",
        "         \"accuracy\":base_eval[\"accuracy\"],\n",
        "         \"precision_macro\":base_eval[\"precision_macro\"],\n",
        "         \"recall_macro\":base_eval[\"recall_macro\"],\n",
        "         \"f1_macro\":base_eval[\"f1_macro\"]},\n",
        "        {\"model\":\"PCA + DT\",\n",
        "         \"accuracy\":pca_eval[\"accuracy\"],\n",
        "         \"precision_macro\":pca_eval[\"precision_macro\"],\n",
        "         \"recall_macro\":pca_eval[\"recall_macro\"],\n",
        "         \"f1_macro\":pca_eval[\"f1_macro\"]}\n",
        "    ])\n",
        "    compare.to_csv(out_dir/\"metrics_compare.csv\", index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n=== Summary ===\")\n",
        "    print(compare.to_string(index=False))\n",
        "    print(f\"\\nConfusion matrices and reports saved to: {out_dir.resolve()}\")\n",
        "    print(\"Files: baseline_report.txt, pca_report.txt, feature_importance_baseline.csv, metrics_compare.csv, confusion_matrix_*.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7fDWhReOZcf",
        "outputId": "f0f407dd-98e8-4743-b48b-d6949048836b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: /content/Lung_Cancer_dataset.csv -> shape (59, 7)\n",
            "Using target column: Result\n",
            "Class distribution:\n",
            "Result\n",
            "0    31\n",
            "1    28\n",
            "Name: count, dtype: int64\n",
            "Numeric cols: 4 | Categorical cols: 2\n",
            "PCA kept components: 33 | Total variance explained: 0.953\n",
            "\n",
            "=== Summary ===\n",
            "      model  accuracy  precision_macro  recall_macro  f1_macro\n",
            "Baseline DT  0.833333         0.875000      0.833333  0.828571\n",
            "   PCA + DT  0.916667         0.928571      0.916667  0.916084\n",
            "\n",
            "Confusion matrices and reports saved to: /content/outputs\n",
            "Files: baseline_report.txt, pca_report.txt, feature_importance_baseline.csv, metrics_compare.csv, confusion_matrix_*.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2hQVmCTcNN-d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac2dc28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3704bf-d571-46df-9e13-93cce1a4b047"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/plotly/datasets/master/lung_cancer_examples.csv -O lung_cancer_examples.csv"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-20 07:01:14--  https://raw.githubusercontent.com/plotly/datasets/master/lung_cancer_examples.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-08-20 07:01:14 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}